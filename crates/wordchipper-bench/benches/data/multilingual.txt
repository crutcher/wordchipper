The quick brown fox jumps over the lazy dog. It's a beautiful day, and I'll be taking my 3 dogs for a walk. Don't forget: the temperature is 72 degrees! We've been waiting since 10:30am.

In 2024, artificial intelligence continued to advance rapidly. Large language models like GPT-4 and Claude demonstrated remarkable capabilities. The researchers couldn't believe the results they'd achieved. "This is unprecedented," said Dr. Smith-Jones, who had been working on the project for 15+ years.

Tokenization is the process of breaking text into smaller units called tokens. For byte-pair encoding (BPE), the algorithm starts with individual bytes and iteratively merges the most frequent adjacent pairs. The vocabulary size determines the granularity: larger vocabularies produce fewer tokens per text but require more memory.

Consider the following edge cases for tokenizers:
  - Multiple   spaces   between   words
  - Tabs		between		fields
  - Mixed	 whitespace	  patterns
  - ALLCAPS SENTENCES WITH NO LOWERCASE
  - camelCaseIdentifiers and snake_case_names
  - URLs like https://example.com/path?query=value&other=123#fragment
  - Email addresses: user@domain.co.uk, first.last+tag@subdomain.example.org
  - File paths: /usr/local/bin/program, C:\Users\Documents\file.txt
  - Numbers: 3.14159, -273.15, 1,000,000, 0xFF, 0b1010, 1e-10, 42nd
  - Currency: $19.99, EUR 1,234.56, JPY 150
  - Dates: 2024-01-15, 01/15/2024, January 15th 2024
  - Times: 14:30:00Z, 2:30 PM EST, T14:30:00+05:30

Mathematical notation and formulas appear frequently in technical text:
f(x) = ax^2 + bx + c, where a != 0
The quadratic formula gives x = (-b +/- sqrt(b^2 - 4ac)) / (2a)
Integration: int_0^inf e^(-x^2) dx = sqrt(pi)/2
Set theory: A = {x in R : x > 0}, |A union B| <= |A| + |B|

def merge_tokens(pairs: list[tuple[int, int]], vocab: dict) -> dict:
    """Merge the most frequent byte pair in the vocabulary."""
    most_frequent = max(pairs, key=pairs.get)
    new_token = len(vocab)
    vocab[most_frequent] = new_token
    # Update all sequences containing this pair
    for seq_id, sequence in enumerate(sequences):
        i = 0
        while i < len(sequence) - 1:
            if (sequence[i], sequence[i + 1]) == most_frequent:
                sequence[i] = new_token
                del sequence[i + 1]
            else:
                i += 1
    return vocab

fn encode_span(text: &str, vocab: &HashMap<Vec<u8>, u32>) -> Vec<u32> {
    let mut tokens: Vec<u32> = text.bytes().map(|b| b as u32).collect();
    loop {
        let best_pair = tokens.windows(2)
            .filter_map(|w| vocab.get(&[w[0] as u8, w[1] as u8]).map(|&t| (w[0], w[1], t)))
            .min_by_key(|&(a, b, _)| vocab[&[a as u8, b as u8]]);
        match best_pair {
            Some((a, b, merged)) => { /* merge */ }
            None => break,
        }
    }
    tokens
}

{"model": "gpt-4o", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "What is the capital of France?"}], "temperature": 0.7, "max_tokens": 150, "top_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0}

<html><head><title>Example Page</title></head><body><div class="container"><h1>Hello, World!</h1><p>This is a <strong>test</strong> of HTML tokenization with &amp; entities &lt;tags&gt; and "quotes".</p><ul><li>Item 1</li><li>Item 2</li><li>Item 3</li></ul></div></body></html>

Contraction stress test: I'm, you're, he's, she's, it's, we're, they're, I've, you've, we've, they've, I'll, you'll, he'll, she'll, we'll, they'll, I'd, you'd, he'd, she'd, we'd, they'd, isn't, aren't, wasn't, weren't, hasn't, haven't, hadn't, doesn't, don't, didn't, won't, wouldn't, shan't, shouldn't, can't, couldn't, mustn't, needn't, let's, that's, who's, what's, where's, when's, why's, how's, there's, here's, it'd, there'd, who'd, what'd.

Repeated patterns: abcabcabcabc, 123123123123, aaabbbccc, xyzxyzxyzxyz
Alternating: aBcDeFgHiJkLmNoPqRsTuVwXyZ, 1a2b3c4d5e6f7g8h9i0j

Unicode Latin extended:
French: Les Etats-Unis d'Amerique sont un pays d'Amerique du Nord. La Revolution francaise a commence en 1789. C'est la vie! Ou est la bibliotheque? Je ne sais pas. L'enfant mange une pomme.
German: Die Osterreichische Nationalbibliothek befindet sich in Wien. Konnen Sie mir bitte helfen? Das ist sehr schon! Ubrigens, der Zurich-See ist wunderschon. Grusse aus Munchen!
Spanish: El nino jugo en el jardin manana. La senorita pidio un cafe con leche. Feliz cumpleanos! Que tal? Como estas? Muy bien, gracias.
Portuguese: A informacao esta disponivel em portugues. Sao Paulo e a maior cidade do Brasil. Obrigado pela atencao!

CJK text samples:
Chinese (Simplified): äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®žè´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦ç ”ç©¶æ–¹å‘ä¹‹ä¸€ã€‚åˆ†è¯æ˜¯ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†çš„åŸºç¡€ä»»åŠ¡ã€‚
Chinese (Traditional): æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ã€‚é€éŽæ¼”ç®—æ³•å’Œçµ±è¨ˆæ¨¡åž‹ï¼Œé›»è…¦ç³»çµ±å¯ä»¥å¾žè³‡æ–™ä¸­å­¸ç¿’ä¸¦åšå‡ºé æ¸¬æˆ–æ±ºç­–ã€‚æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹å­é ˜åŸŸã€‚
Japanese: è‡ªç„¶è¨€èªžå‡¦ç†ï¼ˆã—ãœã‚“ã’ã‚“ã”ã—ã‚‡ã‚Šï¼‰ã¯ã€äººé–“ãŒæ—¥å¸¸çš„ã«ä½¿ã£ã¦ã„ã‚‹è¨€èªžã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«å‡¦ç†ã•ã›ã‚‹æŠ€è¡“ã§ã™ã€‚å½¢æ…‹ç´ è§£æžã€æ§‹æ–‡è§£æžã€æ„å‘³è§£æžãªã©ã®åŸºæœ¬æŠ€è¡“ãŒã‚ã‚Šã¾ã™ã€‚æ±äº¬ã‚¿ãƒ¯ãƒ¼ã¯æ—¥æœ¬ã®æœ‰åãªè¦³å…‰åœ°ã§ã™ã€‚
Korean: ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìžˆìŠµë‹ˆë‹¤. ìžì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ìˆ ìž…ë‹ˆë‹¤. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì€ ì¤‘ìš”í•œ ì „ì²˜ë¦¬ ê³¼ì •ìž…ë‹ˆë‹¤.

Arabic and Hebrew:
Arabic: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø¹Ù„ÙˆÙ… Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù†Ø¸Ù…Ø© Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ù…Ù‡Ø§Ù… ØªØªØ·Ù„Ø¨ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¨Ø´Ø±ÙŠ
Hebrew: ×‘×™× ×” ×ž×œ××›×•×ª×™×ª ×”×™× ×¢× ×£ ×©×œ ×ž×“×¢×™ ×”×ž×—×©×‘ ×”×¢×•×¡×§ ×‘×™×¦×™×¨×ª ×ž×¢×¨×›×•×ª ×—×›×ž×•×ª

Thai: à¸›à¸±à¸à¸à¸²à¸›à¸£à¸°à¸”à¸´à¸©à¸à¹Œà¹€à¸›à¹‡à¸™à¸ªà¸²à¸‚à¸²à¸«à¸™à¸¶à¹ˆà¸‡à¸‚à¸­à¸‡à¸§à¸´à¸—à¸¢à¸²à¸à¸²à¸£à¸„à¸­à¸¡à¸žà¸´à¸§à¹€à¸•à¸­à¸£à¹Œà¸—à¸µà¹ˆà¸¡à¸¸à¹ˆà¸‡à¹€à¸™à¹‰à¸™à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¸£à¸°à¸šà¸šà¸—à¸µà¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹à¸¥à¸°à¸•à¸±à¸”à¸ªà¸´à¸™à¹ƒà¸ˆà¹„à¸”à¹‰

Emoji sequences: Hello ðŸ‘‹ World ðŸŒ! The ðŸš€ launch was ðŸ’¯ amazing ðŸŽ‰ðŸŽŠ! I â¤ï¸ programming ðŸ’». Weather: â˜€ï¸ðŸŒ¤ï¸â›…ðŸŒ¥ï¸â˜ï¸ðŸŒ§ï¸â›ˆï¸ðŸŒ©ï¸. Flags: ðŸ‡ºðŸ‡¸ðŸ‡¬ðŸ‡§ðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡¯ðŸ‡µðŸ‡°ðŸ‡·ðŸ‡¨ðŸ‡³. Family: ðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦ Office: ðŸ‘©â€ðŸ’»ðŸ‘¨â€ðŸ’» Food: ðŸ•ðŸ”ðŸŒ®ðŸ£ðŸœ

Special characters and symbols:
Arrows: -> => <- <= <-> <=> >> << |> <|
Brackets: () [] {} <> (()) [[]] {{}}
Operators: + - * / % ** // += -= *= /= == != >= <= && || !
Punctuation: ... --- *** ___ ### @@@ $$$ %%% ^^^ &&& |||
Typography: "curly quotes" 'single quotes' (parenthetical) [bracketed] {braced}
Math symbols: pi, theta, sigma, delta, epsilon, alpha, beta, gamma

Line ending variations:
Unix line ending
Windows line ending
Mixed endings in one block
	Tab-indented line
    Space-indented line (4 spaces)
        Double-indented (8 spaces)

Empty lines above and below:


  Whitespace-only line above
	Tab-only line above

Long unbroken strings: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
Repeated word: the the the the the the the the the the the the the the the the the the the the
Mixed numbers and text: I have 42 apples, 3.14 pies, and $1,000.00 in my account (ref #A-12345-B).

Base64-like: SGVsbG8gV29ybGQhIFRoaXMgaXMgYSB0ZXN0IG9mIGJhc2U2NCBlbmNvZGluZy4=
Hex-like: 0x48656c6c6f20576f726c6421 0xDEADBEEF 0xCAFEBABE ff00ff 7f7f7f
UUID-like: 550e8400-e29b-41d4-a716-446655440000, a1b2c3d4-e5f6-7890-abcd-ef1234567890

Version strings: v1.0.0, v2.3.4-beta.1, v10.20.30+build.456, >=1.0.0 <2.0.0, ~=3.4, ^0.1.0

Markdown formatting: **bold**, *italic*, ~~strikethrough~~, `code`, [link](url), ![image](src), > blockquote, - list item, 1. numbered item, ### heading, --- separator, ```code block```, | table | row |

Log-like entries:
2024-01-15T14:30:00.123Z INFO  [main] Application started successfully
2024-01-15T14:30:01.456Z DEBUG [worker-3] Processing batch 42/100 (items: 1024)
2024-01-15T14:30:02.789Z WARN  [db-pool] Connection pool utilization at 85% (17/20)
2024-01-15T14:30:03.012Z ERROR [api-handler] Request failed: status=503, latency=2.34s, path=/api/v2/tokens

Stack trace format:
thread 'main' panicked at 'index out of bounds: the len is 10 but the index is 15', src/main.rs:42:5
   0: std::panicking::begin_panic
   1: wordchipper::encoder::encode_span
             at ./src/encoder.rs:156:9
   2: wordchipper::tokenizer::Tokenizer::try_encode
             at ./src/tokenizer.rs:78:9
   3: main
             at ./examples/bench.rs:23:5
