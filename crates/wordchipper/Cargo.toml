[package]
name = "wordchipper"
description = "HPC Rust LLM Tokenizer Library"
keywords = ["ai", "gpt", "bpe", "tokenizer"]
authors.workspace = true
license.workspace = true
repository.workspace = true

version.workspace = true

edition.workspace = true
rust-version.workspace = true


[package.metadata.docs.rs]
all-features = true


[lints]
workspace = true


[features]
default = [
    "foldhash",
    "client",
    "logos",
    "rayon",
]

## Enable the logos DFA word lexer for cl100k/o200k patterns.
logos = ["dep:logos"]

## The base set of features needed to load and run pre-trained encoders and decoders.
client = [
    "download",
    "std",
]

## The download feature enables downloading vocabularies from the internet.
download = [
    "dep:wordchipper-disk-cache",
    "std",
]

## The "std" feature enables the use of the `std` library.
## Without "std", the crate uses hashbrown for HashMap/HashSet.
std = [
    "thiserror/std",
    "dep:base64",
    "dep:strum",
    "dep:strum_macros",
    "fancy-regex/default",
    "foldhash?/std",
    "log/std",
    "num-traits/std",
    "regex/default",
]

## Swaps HashMap/HashSet to ``ahash`` for faster hashing on most modern CPUs.
## If both "ahash" and "foldhash" are enabled, "ahash" wins.
ahash = [
    "dep:ahash",
    "std",
]

## Swaps HashMap/HashSet to ``foldhash`` for faster hashing on most modern CPUs.
## If both "ahash" and "foldhash" are enabled, "ahash" wins.
foldhash = [
    "dep:foldhash",
    "std",
]


## This enables some parallelism wrappers using the ``rayon`` crate.
##
## TODO: I intend on providing a ``tokio`` based ``async`` parallelism mechanism
## as well, to structure more direct ``regex find > encode span`` pipeline parallelism.
rayon = [
    "dep:rayon",
    "std"
]

## This enables a number of ``tracing`` instrumentation points.
## This is only useful for timing tracing of the library itself.
tracing = [
    "tracing/attributes",
]

## Enable test utilities for downstream users.
testing = []



[dependencies]
# macro packages.
cfg-if = { workspace = true }
document-features = { workspace = true }

thiserror = { workspace = true }
logos = { workspace = true, optional = true, features = ["export_derive"] }
fancy-regex = { workspace = true, features = ["unicode"] }
log = { workspace = true }
num-traits = { workspace = true }
regex = { workspace = true, features = ["unicode"] }

# "std" feature deps:
base64 = { workspace = true, optional = true }
strum = { workspace = true, optional = true }
strum_macros = { workspace = true, optional = true }

# Provides HashMap/HashSet in no_std mode (non-optional so `default-features = false` just works).
hashbrown = { workspace = true, features = ["alloc"] }

spin = { workspace = true }
once_cell = { workspace = true, features = ["alloc"] }

ahash = { workspace = true, optional = true }
foldhash = { workspace = true, optional = true }

# "download" feature deps:
wordchipper-disk-cache = { version = "0.7.3", path = "../wordchipper-disk-cache", optional = true }

# "rayon" feature deps:
rayon = { workspace = true, optional = true }

# "tracing" feature deps:
tracing = { workspace = true, optional = true }


[dev-dependencies]
tempdir = { workspace = true }
tiktoken-rs = { workspace = true }
tokenizers = { workspace = true, features = ["http"] }
serial_test = { workspace = true }
proptest = { workspace = true }
